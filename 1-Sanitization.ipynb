{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXmcb7wBzcTU"
   },
   "source": [
    "# AI Tuning: Sanitization Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpMDLjKzzcTY"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Presidio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U presidio_anonymizer\n",
    "!pip install -U presidio_analyzer\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports propre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cYpdVbF1n7V6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer.entities.engine import OperatorConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TviSDz8OzcTb"
   },
   "source": [
    "# Raw Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dYyXtUJo2zT"
   },
   "outputs": [],
   "source": [
    "# change variables per customer\n",
    "FILENAME = \"kroger-post-prod-utterance.csv\"\n",
    "IVA = 'kroger'\n",
    "\n",
    "# import data into dataframe\n",
    "data = pd.read_csv(FILENAME)\n",
    "\n",
    "# find date range\n",
    "min_date, max_date = data['timestamp'].min(), data['timestamp'].max()\n",
    "print(\"Min date: \", min_date)\n",
    "print(\"Max data: \", max_date)\n",
    "\n",
    "DATE = '.'.join([\n",
    "    min_date.split()[0].split('-')[1],\n",
    "    min_date.split()[0].split('-')[2], \n",
    "    min_date.split()[0].split('-')[0]\n",
    "])\n",
    "\n",
    "DATE += '-' + '.'.join([\n",
    "    max_date.split()[0].split('-')[1],\n",
    "    max_date.split()[0].split('-')[2], \n",
    "    max_date.split()[0].split('-')[0]\n",
    "])\n",
    "\n",
    "print(DATE)\n",
    "\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# save shape\n",
    "original_data_rows = data.shape[0]\n",
    "\n",
    "# view df\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-0zvIwszcTc"
   },
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVAtLoLFpS8W"
   },
   "outputs": [],
   "source": [
    "# grab only columns of interest\n",
    "data = data[['timestamp','Intent', 'Utterance', 'IntentConfidence']]\n",
    "#data = data[['timestamp','Correct_Intent', 'Utterance_Trascription']]\n",
    "\n",
    "# change column names to utterance and intent\n",
    "data = data.rename(columns={'Utterance': 'Raw Utterance', 'Intent': 'Intent', 'timestamp': 'Timestamp', 'IntentConfidence': 'Confidence'})\n",
    "#data = data.rename(columns={'Utterance_Trascription': 'Raw Utterance', 'Correct_Intent': 'Intent', 'timestamp': 'Timestamp'})\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# view dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qhpk04RzcTe"
   },
   "source": [
    "## Timestamp Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "h6R7vnjcpeZI"
   },
   "outputs": [],
   "source": [
    "# new data frame with split value columns\n",
    "new = data[\"Timestamp\"].astype(str).str.split(\" \", n = 1, expand = True)\n",
    "\n",
    "# making separate first name column from new data frame\n",
    "data[\"Date\"]= new[0]\n",
    "\n",
    "# drop old timestamp column\n",
    "data.drop(columns =[\"Timestamp\"], inplace = True)\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# view dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9Gm2p8GzcTf"
   },
   "source": [
    "## NAs Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "wsezeQmcqLDe"
   },
   "outputs": [],
   "source": [
    "# row count before\n",
    "before = data.shape[0]\n",
    "\n",
    "# pd doesn't recognise empty strings as null, we need to convert them to NaN using numpy\n",
    "data['Raw Utterance'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# drop NA's\n",
    "data.dropna(subset=['Raw Utterance'], inplace=True)\n",
    "\n",
    "# row count after\n",
    "after = data.shape[0]\n",
    "\n",
    "# difference\n",
    "diff = before - after\n",
    "\n",
    "# print results\n",
    "print(\"# of rows before dropping NA: \", before)\n",
    "print(\"# of NA values: \", diff)\n",
    "print(\"# of rows after dropping NA: \", after)\n",
    "\n",
    "# save shape\n",
    "non_empty_data_rows = after\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", data.shape)\n",
    "\n",
    "# view dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWMKEIVQzcTf"
   },
   "source": [
    "## Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "jge688SPqY6z"
   },
   "outputs": [],
   "source": [
    "# lowercase the text\n",
    "data['Lowercase'] = data['Raw Utterance'].astype(str).str.lower()\n",
    "\n",
    "# rename dataframe\n",
    "usable_data = data\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", usable_data.shape)\n",
    "\n",
    "# view dataframe\n",
    "usable_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcTPcE27zcTg"
   },
   "source": [
    "## Text Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SzNHO6HztDCB"
   },
   "outputs": [],
   "source": [
    "# set up the NER tagging tool\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# set up anonymization tool\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "# define function to apply to dataframe for anonymization\n",
    "def analyze_and_anonymize_text(text):\n",
    "\n",
    "    # dictionary assigning replacement values for each label assigned to entities\n",
    "    operators = {\n",
    "        \"PERSON\": OperatorConfig(operator_name=\"replace\", params={\"new_value\": \"<PERSON_NAME>\"}),\n",
    "        \"LOCATION\": OperatorConfig(operator_name=\"replace\", params={\"new_value\": \"<LOCATION_NAME>\"}),\n",
    "        \"DATE_TIME\": OperatorConfig(operator_name=\"replace\", params={\"new_value\": \"<DATE_TIME>\"}),\n",
    "        \"CREDIT_CARD\": OperatorConfig(operator_name=\"replace\", params={\"new_value\": \"<CREDIT_CARD>\"}),\n",
    "        \"PHONE_NUMBER\": OperatorConfig(operator_name=\"replace\", params={\"new_value\": \"<PHONE_NUMBER>\"}),\n",
    "        \"EMAIL_ADDRESS\": OperatorConfig(operator_name=\"replace\", params={\"new_value\": \"<EMAIL_ADDRESS>\"})\n",
    "    }\n",
    "\n",
    "    #tagging results for Dates/Times, Location, Person, Phone Number, Credit Card\n",
    "    results = analyzer.analyze(text=text, entities=['DATE_TIME','LOCATION','PERSON','PHONE_NUMBER','CREDIT_CARD', 'EMAIL_ADDRESS'],language='en')\n",
    "\n",
    "    #anonymize text\n",
    "    anonymized_text = anonymizer.anonymize(text = text, analyzer_results = results, operators = operators).text\n",
    "\n",
    "    # return\n",
    "    return anonymized_text\n",
    "\n",
    "# call function\n",
    "usable_data['Clean'] = usable_data['Lowercase'].apply(lambda x: analyze_and_anonymize_text(x))\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", usable_data.shape)\n",
    "\n",
    "# view dataframe\n",
    "usable_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDYCuFI9zcTg"
   },
   "source": [
    "## Digits Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AwYYow7YTmhZ"
   },
   "outputs": [],
   "source": [
    "def tag_digits(text):\n",
    "\n",
    "    # skip replacing \"401k\"\n",
    "    if \"401k\" in text:\n",
    "        return text\n",
    "\n",
    "    # skip replacing \"401K\"\n",
    "    elif \"401K\" in text:\n",
    "        return text\n",
    "\n",
    "    # return\n",
    "    else:\n",
    "        return re.sub(r'\\d+', '<DIGITS>', text)\n",
    "\n",
    "# call function\n",
    "usable_data['Clean'] = usable_data['Clean'].apply(tag_digits)\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", usable_data.shape)\n",
    "\n",
    "# view dataframe\n",
    "usable_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQmpF5pozcTh"
   },
   "source": [
    "## Redacted Rows Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pQ1iDD7Cuv6E"
   },
   "outputs": [],
   "source": [
    "# row count before\n",
    "before = usable_data.shape[0]\n",
    "\n",
    "# drop redacted utterances\n",
    "usable_data = usable_data[~usable_data.apply(lambda row: row.astype(str).str.contains(r'^<.*>$').any(), axis=1)]\n",
    "\n",
    "# row count after\n",
    "after = usable_data.shape[0]\n",
    "\n",
    "# difference\n",
    "diff = before - after\n",
    "\n",
    "# print results\n",
    "print(\"# of rows before dropping spaces: \", before)\n",
    "print(\"# of rows containing only redacted info: \", diff)\n",
    "print(\"# of rows after dropping spaces: \", after)\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", usable_data.shape)\n",
    "\n",
    "# view dataframe\n",
    "usable_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AshcoDBZzcTh"
   },
   "source": [
    "## Undefined Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "T4AkkMPeu60s"
   },
   "outputs": [],
   "source": [
    "# create new dataframe\n",
    "data_for_analysis = usable_data.copy()\n",
    "\n",
    "# define unnamed intents\n",
    "data_for_analysis.Intent = data_for_analysis.Intent.fillna('Undefined')\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", data_for_analysis.shape)\n",
    "\n",
    "# view dataframe\n",
    "data_for_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4-on20eJPzOj"
   },
   "outputs": [],
   "source": [
    "# @title Check Before Export\n",
    "\n",
    "throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjYs3jZ2zcTi"
   },
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdPq_aKMzcTi"
   },
   "source": [
    "## Export File to Sample for HF\n",
    "### This file contains stopwords and is not lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_0teOEyZvCIH"
   },
   "outputs": [],
   "source": [
    "# assemble filenames\n",
    "filename = (IVA + \"_HF_sanitised_data_to_sample_\" + DATE + \".csv\")\n",
    "\n",
    "# select columns of interest\n",
    "sanitized_data_hf = data_for_analysis[['Clean', 'Intent']]\n",
    "\n",
    "# sort by intent\n",
    "sanitized_data_hf = sanitized_data_hf.sort_values(by = 'Intent')\n",
    "\n",
    "# save sanitized data\n",
    "sanitized_data_hf.to_csv(filename, index = False)\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", sanitized_data_hf.shape)\n",
    "\n",
    "# save shape\n",
    "clean_data_rows = sanitized_data_hf.shape[0]\n",
    "\n",
    "# view dataframe\n",
    "sanitized_data_hf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z95C3j9szcTi"
   },
   "source": [
    "## Export General Sanitized Data\n",
    "### This file does not contain stopwords, and has undergone lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCCKNnKCzcTi"
   },
   "outputs": [],
   "source": [
    "# load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# create custom list of stop words\n",
    "stop_words.extend(['PERSON_NAME', 'LOCATION_NAME', 'DATE_TIME', 'CREDIT_CARD', 'PHONE_NUMBER', 'EMAIL_ADDRESS', 'DIGITS', 'DIGITSth', 'DIGITSst', 'DIGITSrd', 'right', 'take', 'look', 'wait', 'let', 'see', 'dollars', 'zip', 'code', 'alright', 'call', 'know', 'stay', 'line', 'give', 'moment', 'west', 'thirty', 'street', 'going', 'really', 'appreciate', 'back', 'tone', 'get', 'much', 'could', 'check', 'sure', 'left', 'hang', 'press', 'please', 'leave', 'message', 'gon', 'na', 'can', 'not', 'gmail', 'com', 'finished', 'recording', 'okay', 'yeah', 'may', 'yes', 'dot', 'email','said', 'go', 'got', 'okai', 'like', 'number', 'record', 'message', 'twenty', 'hundred', 'seventy', 'forty', 'sixteen', 'sir', 'miss', 'voice', 'good', 'day', 'help', 'today', 'custom', 'care', 'thank','calling', 'hello', 'hi', 'welcome', 'thank', 'bye', 'goodbye', 'name', 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5trZGdfzcTj"
   },
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "data_for_analysis['Tokens'] = data_for_analysis['Clean'].apply(tokenizer.tokenize)\n",
    "\n",
    "# remove stopwords defined above\n",
    "data_for_analysis['Tokens'] = data_for_analysis['Tokens'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# convert output to a string and only keep words longer than 2 chars\n",
    "data_for_analysis['Tokens'] = data_for_analysis['Tokens'].apply(lambda x: ' '.join([item for item in x if len(item) > 2]))\n",
    "\n",
    "# lemmatize tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_for_analysis['Lemmatized'] = data_for_analysis['Tokens'].apply(lemmatizer.lemmatize)\n",
    "data_for_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB8utV0lvQUf"
   },
   "outputs": [],
   "source": [
    "# assemble filenames\n",
    "filename = (IVA + \"_sanitized_data_for_EDA_\" + DATE + \".csv\")\n",
    "\n",
    "# select columns of interest\n",
    "sanitized_data = data_for_analysis[['Lemmatized', 'Intent', 'Date', 'Confidence']]\n",
    "#sanitized_data = data_for_analysis[['Clean', 'Intent', 'Date', 'Lemmatized']]\n",
    "\n",
    "# create a deep copy to avoid warnings\n",
    "sanitized_data = sanitized_data.copy()\n",
    "\n",
    "# drop empty rows created as a result of lemmatization\n",
    "# pd doesn't recognise empty strings as null, we need to convert them to NaN using numpy\n",
    "sanitized_data['Lemmatized'].replace('', np.nan, inplace=True)\n",
    "\n",
    "# drop NA's\n",
    "sanitized_data.dropna(subset=['Lemmatized'], inplace=True)\n",
    "\n",
    "# row count after\n",
    "after_lemmatization = sanitized_data.shape[0]\n",
    "print(\"# of rows after dropping NA: \", after_lemmatization)\n",
    "\n",
    "# rename columns\n",
    "sanitized_data = sanitized_data.rename(columns = {'Lemmatized':'Utterance'})\n",
    "\n",
    "# sort by intent\n",
    "sanitized_data = sanitized_data.sort_values(by = 'Intent')\n",
    "\n",
    "# save sanitized data\n",
    "sanitized_data.to_csv(filename, index = False)\n",
    "\n",
    "# view shape\n",
    "print(\"Shape:\", sanitized_data.shape)\n",
    "\n",
    "# save shape\n",
    "clean_data_rows = sanitized_data.shape[0]\n",
    "\n",
    "# view dataframe\n",
    "sanitized_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCh5qZtczcTj"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1F3tdFF1QPaZ"
   },
   "outputs": [],
   "source": [
    "# print basic metrics\n",
    "print(f'Number of rows in original file: {original_data_rows}.')\n",
    "print(f'Number of rows containing an utterance: {non_empty_data_rows}.')\n",
    "print(f'Number of empty rows dropped: {original_data_rows - non_empty_data_rows}.')\n",
    "print(f'Number of rows in pre-processed file: {after_lemmatization}.')\n",
    "print(f'Number of rows dropped as a result of the cleaning process: {non_empty_data_rows - after_lemmatization}.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
